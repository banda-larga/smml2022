{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJxLL7xckPKt"
      },
      "source": [
        "# Cat-Dog Classification with Neural Networks - can we reach SOTA?\n",
        "\n",
        "\n",
        "**Exam:**\n",
        "Statistical Methods for Machine Learning  \n",
        "**Author:** \n",
        "Edoardo Federici\n",
        "\n",
        "We will implement two architectures: \n",
        "we start with a simple [CNN](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). Then we'll train a [ViT]() (vision transformer). We will compare results with ConvNets, and then discuss recent advances in CV with the paper about [ConvNeXt]() - 'a convnet for the 2020s', fine-tuning a base (large and xlarge are a no-go) model on the dataset reaching public SOTA on **Cats_vs_Dogs**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFuAt7kPkPKy"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "**Note:** After CNN and ViT, the code is in PyTorch for the ConvNeXt (Is from Facebook AI. I've tried loading and fine-tuning the [Tensorflow weights](https://www.tensorflow.org/api_docs/python/tf/keras/applications/convnext/ConvNeXtBase) but they don't perform as good as the PyTorch ones).\n",
        "\n",
        "\n",
        "_It was done to learn as much as I could, since dealing with multiple libraries seems to be the standard, today._\n",
        "\n",
        "\n",
        "When dealing with new architectures we often deal with pre-trained models. Pre-training is an expensive task. Recent checkpoints are sadly not all available in PyTorch or Tensorflow. Google side of the Research Community is mostly now using JAX. In a world with giant models and datasets, compute time is an highly valuable resource. Flax reduces training time drastically. It is also 'accelerator indipendent', so it can run on TPUs, GPUs or CPUs without headaces.\n",
        "\n",
        "\n",
        "_ViTs are pre-trained in Flax, Tensorflow-ported weights may have produced lower scores_\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo3e9PH2VbpF"
      },
      "source": [
        "For the experiments we used the _T4_ available on Colab and a _NVIDIA RTX 3060 OC 12GB_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkEUx5i-0ewA",
        "outputId": "33f81be2-cf31-4650-e058-88b398065bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 18 01:44:02 2022       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
            "|  0%   50C    P8    19W / 170W |     11MiB / 12288MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|    0   N/A  N/A      2128      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
            "|    0   N/A  N/A      2443      G   ...ome-remote-desktop-daemon        2MiB |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCWutq3cVYTe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow_addons # for AdamW\n",
        "!pip install transformers datasets "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow_datasets"
      ],
      "metadata": {
        "id": "OtJxfyQbxe1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc \n",
        "import numpy as np \n",
        "import random\n",
        "import json\n",
        "import time\n",
        "import itertools\n",
        "import functools \n",
        "from statistics import mean\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import glob\n",
        "import warnings\n",
        "from typing import Any, Tuple, List, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXFptie3xxgQ",
        "outputId": "971178c5-a285-450c-94d3-b1a50a8bd525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/edoardo/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTFjUMvtezob"
      },
      "source": [
        "# 🔧 **Dataset**\n",
        "\n",
        "Cats and Dogs dataset from [here](https://unimibox.unimi.it/index.php/s/eNGYGSYmqynNMqF).\n",
        "\n",
        "- removing faulty images \n",
        "- renaming columns etc.\n",
        "\n",
        "> We have two classes: _**Cat**_ and _**Dog**_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh0M6hSD_2Gt"
      },
      "source": [
        "**image_dataset_from_directory** will return a [tf.data.Dataset]() that yields batches of images from the subdirectories classes.\n",
        "\n",
        "**Faster reading speed?**\n",
        "\n",
        "Prefetching allow us to read the data for step _s+1_ while the model is computing step _s_. This reduces reading time. \n",
        "\n",
        "Caching in memory or on local storage allow us to reduce data reading time. (Using those eats a lot of RAM - which causes fine-tuning on the Keras ConvNeXt / ViT impossible on Colab, skip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-X5Bd-g9icK"
      },
      "outputs": [],
      "source": [
        "def dataset_from_directory(path, data_args):\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        path,\n",
        "        validation_split=data_args.train_test_split,\n",
        "        subset=\"training\",\n",
        "        color_mode=data_args.mode,\n",
        "        seed=data_args.seed,\n",
        "        image_size=data_args.image_size,\n",
        "        batch_size=data_args.train_batch_size,\n",
        "        interpolation='bilinear', \n",
        "        label_mode=data_args.label\n",
        "    )\n",
        "\n",
        "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        path,\n",
        "        validation_split=data_args.train_test_split,\n",
        "        subset=\"validation\",\n",
        "        color_mode=data_args.mode,\n",
        "        seed=data_args.seed,\n",
        "        image_size=data_args.image_size,\n",
        "        batch_size=data_args.test_batch_size,\n",
        "        interpolation='bilinear',\n",
        "        label_mode=data_args.label\n",
        "    )\n",
        "\n",
        "    # this causes a memory leak, sadly\n",
        "    # train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "    # test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSBfo8Kq_okh"
      },
      "source": [
        "**Cleaning pipeline**\n",
        "\n",
        "Faulty images will cause problems when we train our models. So it's important to clean our folder before. Since we later deal with Torch, we are gonna spot errors with tf.image and with PIL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlTSuIqu3LKC"
      },
      "outputs": [],
      "source": [
        "def rm_faulty_images(path: str = None):\n",
        "    warnings.filterwarnings(\n",
        "        \"ignore\", \n",
        "        \"(Possibly )?corrupt EXIF data\", \n",
        "        UserWarning\n",
        "    )\n",
        "\n",
        "    warnings.filterwarnings(\n",
        "        \"ignore\", \n",
        "        \"extraneous bytes before marker\", \n",
        "        UserWarning\n",
        "    )\n",
        "     \n",
        "    img_paths = glob.glob(os.path.join(path,'*/*.*'))\n",
        "    faulty_images = []\n",
        "\n",
        "    print(\"Bad paths:\")\n",
        "    for image_path in img_paths:\n",
        "        try:\n",
        "            # we deal also with PIL for torch\n",
        "            # (tf skips jpgs that causes training issues)\n",
        "            img_bytes = tf.io.read_file(image_path)\n",
        "            _ = tf.image.decode_image(img_bytes)\n",
        "            _ = Image.open(image_path)\n",
        "        except (\n",
        "            tf.errors.InvalidArgumentError, \n",
        "            UnidentifiedImageError\n",
        "        ) as e:\n",
        "            print(f\"- Found bad path at: {image_path} - {e}\")\n",
        "            faulty_images.append(image_path)\n",
        "\n",
        "    if len(faulty_images) == 0:\n",
        "        print(\"Images are ok!\")\n",
        "    else:\n",
        "        print(\"Removing them...\")\n",
        "        for path in faulty_images:\n",
        "            print(f\"- Removing image: {path}\")\n",
        "            os.system(f\"rm -rf {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkhoNc7A0wFf"
      },
      "outputs": [],
      "source": [
        "def cats_vs_dogs(args, get_only=False):\n",
        "    print(\n",
        "        \" Dataset:    Cats_vs_Dogs\\n\" \n",
        "        \" Images:     ~25000\\n\" \n",
        "        f\" Image_size: {args.image_size}\\n\"\n",
        "        f\" Color_mode: {args.mode}\\n\"\n",
        "    )\n",
        "\n",
        "    name = \"cats_vs_dogs.zip\"\n",
        "    img_folder = \"Cats_vs_Dogs\"\n",
        "    url = \"https://unimibox.unimi.it/index.php/s/eNGYGSYmqynNMqF/download\"\n",
        "\n",
        "    target_dir = os.getcwd() + f\"/{args.data_path}\"\n",
        "    target = f\"{target_dir}/{name}\"\n",
        "    target_imgs = f\"{target_dir}/{img_folder}\"\n",
        "\n",
        "    if not os.path.exists(target_dir):\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "        print(\"Downloading...\")\n",
        "        os.system(f\"curl {url} -o {target}\")\n",
        "        print(f\"Dataset downloaded to {target}\")\n",
        "    \n",
        "    if not os.path.exists(target_imgs):\n",
        "        # extract all files\n",
        "        os.system(f\"unzip -qq {target} -d {target_dir}\")\n",
        "        print(f\"Dataset extracted!\")\n",
        "\n",
        "        # rename folders to simplify things later \n",
        "        os.system(f\"mv {target_dir}/CatsDogs {target_imgs}\")\n",
        "        os.system(f\"mv {target_imgs}/Cats {target_imgs}/Cat\")\n",
        "        os.system(f\"mv {target_imgs}/Dogs {target_imgs}/Dog\")\n",
        "\n",
        "    rm_faulty_images(target_imgs)\n",
        "    if not get_only:\n",
        "        train, test = dataset_from_directory(target_imgs, args)\n",
        "        return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-tCgGq36qyo"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_path: str = \"dataset\"\n",
        "    image_size: Tuple[int] = (224, 224)\n",
        "    train_batch_size: int = 32\n",
        "    test_batch_size: int = 32\n",
        "    seed: int = 42 # so long, and thanks for all the fish\n",
        "    train_test_split: float = 0.2\n",
        "    mode: str = 'rgb'\n",
        "    label: str = 'categorical'\n",
        "\n",
        "data_args = DataArguments()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-91F7QuaPB2i"
      },
      "source": [
        "# Dataset for Keras Models\n",
        "\n",
        "- (_LeNet5_ & _ViT_)\n",
        "\n",
        "With Keras we use [TensorFlow Datasets](https://www.tensorflow.org/datasets). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgp5OrhzCF5S"
      },
      "source": [
        "The server is slow, so it'll take some time to load. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mR6I3fY2FY1"
      },
      "outputs": [],
      "source": [
        "train_ds, test_ds = cats_vs_dogs(data_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLDMoswXAY0_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "class_names = {0: \"Cat\", 1: \"Dog\"}\n",
        "\n",
        "for images, labels in train_ds.skip(1).take(1):\n",
        "    for i in range(16):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[np.argmax(labels[i].numpy())])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvANDXJqs5DT"
      },
      "source": [
        "## **Image Augmentation** and **Normalization**\n",
        "\n",
        "An important step before starting training is Image Augmentation. When dealing with images (and with text), we can think that:\n",
        "> more images -> better model\n",
        "\n",
        "Data Augmentation reduces the chanches of _overfitting_ and is a general best practice when dealing with ConvNets because helps improve generalization. We use several augmentations: \n",
        "- **random zoom**\n",
        "- **random flip**\n",
        "- **random rotation**\n",
        "\n",
        "By normalizing the data to a uniform mean of 0 and a standard deviation of 1, faster convergence is achieved. We rescale values from _[0, 1]_ but we can also choose _[-1, 1]_ using:\n",
        "\n",
        "```\n",
        "tf.keras.layers.Rescaling(scale=1./127.5, offset=-1)\n",
        "```\n",
        "We apply augmentation only on the train dataset, we don't want all those changes for evaluation (we apply only a small horizontal flip). \n",
        "\n",
        "If you're using Tensorflow version 2.9.1 you can also use random brightness etc. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRci1X7wBIHL"
      },
      "outputs": [],
      "source": [
        "train_pipeline = tf.keras.Sequential(\n",
        "    [   \n",
        "        tf.keras.layers.RandomZoom(\n",
        "            height_factor=0.4, width_factor=0.4\n",
        "        ),\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(0.1), \n",
        "        tf.keras.layers.Normalization(mean=0., variance=1.),  \n",
        "        tf.keras.layers.Rescaling(scale=1./255),   \n",
        "    ]\n",
        ")\n",
        "\n",
        "test_pipeline = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.Normalization(mean=0., variance=1.),\n",
        "        tf.keras.layers.Rescaling(scale=1./255),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woLRFiu0w7ou"
      },
      "source": [
        "Let’s now see how random augmentation affect images from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO5YmGkpEgvu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "class_names = {0: \"Cat\", 1: \"Dog\"}\n",
        "\n",
        "for images, labels in train_ds.skip(1).take(1):\n",
        "    images = train_pipeline(images)\n",
        "    for i in range(16):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow((images[i].numpy()*255).astype(\"uint8\"))\n",
        "        plt.title(class_names[np.argmax(labels[i].numpy())])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIHqi0N_kFiv"
      },
      "source": [
        "## _K-Fold_\n",
        "\n",
        "We use K-Fold Cross-Validation. Our Train set is splitted into n-folds. One fold is used as Validation, remaining folds are joined to create a i-th train set. It's not the cleanest from-scratch implementation but it does the job. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYPUAPQ5SSHF"
      },
      "outputs": [],
      "source": [
        "# partially adapted from \n",
        "# https://github.com/fenwickslab/fenwicks/blob/2d539634efe829bd5a2e2c1b9c837819a89bb707/data.py#L370\n",
        "\n",
        "class KFold:\n",
        "    def __init__(\n",
        "        self, \n",
        "        dataset, \n",
        "        n_folds: Optional[int] = 5, \n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.n_folds = n_folds\n",
        "        self.ind = iter(range(n_folds+1))\n",
        "\n",
        "    def get_fold(self, val_fold_idx, training):\n",
        "        if training:\n",
        "            trn_shards = itertools.chain(\n",
        "                range(val_fold_idx), \n",
        "                range(val_fold_idx + 1, self.n_folds)\n",
        "            )\n",
        "            update_func = lambda ds, i: ds.concatenate(self.dataset.shard(self.n_folds, i))\n",
        "            dataset = functools.reduce(\n",
        "                update_func, \n",
        "                trn_shards, \n",
        "                self.dataset.shard(self.n_folds, next(trn_shards))\n",
        "            )\n",
        "        else:\n",
        "            dataset = self.dataset.shard(self.n_folds, val_fold_idx)\n",
        "        return dataset\n",
        "\n",
        "    def __getitem__(self, _):\n",
        "        index = next(self.ind)\n",
        "        train = self.get_fold(index, True)\n",
        "        val = self.get_fold(index, False)\n",
        "        return train, val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQlSk1nPfTeG"
      },
      "source": [
        "# 🕰 **LeNet5** \n",
        "\n",
        "Lenet-5 is one of the earliest pre-trained models proposed by Yann LeCun and others in the year 1998, in the research paper [Gradient-Based Learning Applied to Document Recognition](). They used this architecture for recognizing the handwritten and machine-printed characters.\n",
        "\n",
        "The main reason behind the popularity of this model was its simple and straightforward architecture. It is a multi-layer convolution neural network for image classification.\n",
        "\n",
        "- three sets of **convolutions**\n",
        "- two **dense layers**\n",
        "\n",
        "Paper: [Gradient-Based Learning Applied to Document Recognition]()\n",
        "\n",
        "Abstract: _**Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique**. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. **Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques**._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on93GCZyf9Q-"
      },
      "outputs": [],
      "source": [
        "print(\"Tensorflow:\", tf.__version__)\n",
        "\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from datetime import datetime \n",
        "from keras.layers import (\n",
        "    Dense, \n",
        "    Flatten, \n",
        "    Conv2D, \n",
        "    MaxPooling2D,\n",
        "    Dropout, \n",
        "    BatchNormalization\n",
        ") \n",
        "import keras as k\n",
        "from keras import layers\n",
        "from keras import Model\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow_addons.optimizers import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "928isnxjhNN7"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class LeNetTrainingArgs:\n",
        "    model_name_or_path: str = \"lenet\"\n",
        "    monitor_best: str = \"val_loss\"\n",
        "    epochs: int = 10\n",
        "    n_folds: int = 5\n",
        "    learning_rate: float = 0.001\n",
        "    dropout: float = 0.2\n",
        "    jit_compile: bool = False\n",
        "    log_dir: str = \"lenet/logdir/\"\n",
        "\n",
        "ln_args = LeNetTrainingArgs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7IvqBeKkkyW"
      },
      "outputs": [],
      "source": [
        "kfold = KFold(train_ds, n_folds=ln_args.n_folds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a '_modernized_' version of LeNet:\n",
        "\n",
        "- **TanH** is replaced with **ReLU**\n",
        "- ReLU exploding gradients can be solved with **He initialization**\n",
        "- **Dropout** was added before the bigger dense layer, which helps reducing overfitting \n",
        "- **Batch Normalization** was added"
      ],
      "metadata": {
        "id": "oSD3lkhbbFox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPYW-u0mhRCO"
      },
      "outputs": [],
      "source": [
        "class LeNet(Model):\n",
        "    def __init__(self, \n",
        "            dropout: float = 0.4, \n",
        "            image_size: int = 224, \n",
        "            num_ch: int = 3\n",
        "    ):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = Conv2D(\n",
        "            32, 5, padding=\"same\",\n",
        "\t\t\tinput_shape=(image_size, image_size, num_ch), \n",
        "            activation='relu', \n",
        "            kernel_initializer='he_uniform'\n",
        "        )\n",
        "        self.max_pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
        "        self.batch_norm1 = BatchNormalization()\n",
        "\n",
        "        self.conv2 = Conv2D(\n",
        "            64, 5, padding=\"valid\", \n",
        "            activation='relu', \n",
        "            kernel_initializer='he_uniform'\n",
        "        )\n",
        "        self.max_pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
        "        self.batch_norm2 = BatchNormalization()\n",
        "\n",
        "        self.conv3 = Conv2D(\n",
        "            64, 5, padding=\"valid\", \n",
        "            activation='relu', \n",
        "            kernel_initializer='he_uniform'\n",
        "        )\n",
        "        self.max_pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
        "        self.batch_norm3 = BatchNormalization()\n",
        "\n",
        "        # this produces a vector |v| = 1600\n",
        "        self.flatten = Flatten()\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "        # dense layers for classification\n",
        "        self.d1 = Dense(256, activation='relu', kernel_initializer='he_uniform')\n",
        "        self.d2 = Dense(2, activation='softmax')\n",
        "    \n",
        "    def call(self, x, training=False):\n",
        "        x = self.max_pool1(self.conv1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "\n",
        "        x = self.max_pool2(self.conv2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "\n",
        "        x = self.max_pool3(self.conv3(x))\n",
        "        x = self.batch_norm3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        if training:\n",
        "            x = self.dropout(x, training=training)\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhDJLKodn5DB"
      },
      "outputs": [],
      "source": [
        "loss_fn = k.losses.CategoricalCrossentropy()\n",
        "\n",
        "train_acc = k.metrics.CategoricalAccuracy()\n",
        "test_acc = k.metrics.CategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Choose the optimizer:***"
      ],
      "metadata": {
        "id": "Af6JnBWAr3lB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krKDoQCS9xsi"
      },
      "outputs": [],
      "source": [
        "# adamw\n",
        "def get_model():\n",
        "    lr_schedule = tf.optimizers.schedules.ExponentialDecay(1e-3, 100, 0.9)\n",
        "    wd_schedule = tf.optimizers.schedules.ExponentialDecay(5e-5, 100, 0.9)\n",
        "\n",
        "    opt = AdamW(learning_rate=lr_schedule, weight_decay=lambda: None)\n",
        "    opt.weight_decay = lambda: wd_schedule(opt.iterations)\n",
        "\n",
        "    model = LeNet(\n",
        "        dropout = ln_args.dropout, \n",
        "        image_size = data_args.image_size[0], \n",
        "        num_ch = 3\n",
        "    )\n",
        "\n",
        "    return model, opt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sgd momentum \n",
        "def get_model():\n",
        "    lr_schedule = tf.optimizers.schedules.ExponentialDecay(1e-3, 100, 0.9)\n",
        "\n",
        "    opt = tf.keras.optimizers.SGD(\n",
        "        learning_rate=lr_schedule, momentum=0.9, nesterov=False, name=\"SGD\"\n",
        "    )\n",
        "\n",
        "    model = LeNet(\n",
        "        dropout = ln_args.dropout, \n",
        "        image_size = data_args.image_size[0], \n",
        "        num_ch = 3\n",
        "    )\n",
        "\n",
        "    return model, opt"
      ],
      "metadata": {
        "id": "-Q3MLeBBxr8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adam\n",
        "def get_model():\n",
        "    lr_schedule = tf.optimizers.schedules.ExponentialDecay(1e-3, 100, 0.9)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=lr_schedule,\n",
        "    )\n",
        "\n",
        "    model = LeNet(\n",
        "        dropout = ln_args.dropout, \n",
        "        image_size = data_args.image_size[0], \n",
        "        num_ch = 3\n",
        "    )\n",
        "\n",
        "    return model, opt"
      ],
      "metadata": {
        "id": "rRfcXUnVImU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Test step:**"
      ],
      "metadata": {
        "id": "Mm18dJDdsANB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nltTYt23myqg"
      },
      "outputs": [],
      "source": [
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(images, training=True)\n",
        "        loss_value = loss_fn(labels, logits)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc.update_state(labels, logits)\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "def test_step(images, labels):\n",
        "    val_logits = model(images, training=False)\n",
        "    loss_value = loss_fn(labels, val_logits)\n",
        "    test_acc.update_state(labels, val_logits)\n",
        "\n",
        "    predicted = np.argmax(val_logits, axis=-1)\n",
        "    labels = np.argmax(labels, axis=-1)\n",
        "    zo_loss = zero_one_loss(labels, predicted)\n",
        "\n",
        "    return zo_loss, loss_value.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeQuFAV9gfie"
      },
      "source": [
        "**Training loop**: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P2vF44E4XNF"
      },
      "outputs": [],
      "source": [
        "num_train_examples = train_ds.cardinality().numpy()\n",
        "steps_per_epoch = num_train_examples // ((ln_args.n_folds-1) * data_args.train_batch_size)\n",
        "epochs = ln_args.epochs\n",
        "\n",
        "print(\"Running training:\")\n",
        "print(f\"Total examples from train_ds = {num_train_examples}\")\n",
        "print(f\"Num Epochs for each fold = {epochs}\")\n",
        "print(f\"Train batch size = {data_args.train_batch_size}\")\n",
        "print(f\"Num folds = {ln_args.n_folds}\\n\")\n",
        "\n",
        "for fold, (train_fold, test_fold) in enumerate(kfold):\n",
        "    num_fold_examples = train_fold.cardinality().numpy()\n",
        "    print(f\"\\nFold: {fold+1} \", \"=\"*10)\n",
        "    print(f\"Total fold examples: {num_fold_examples}\")\n",
        "\n",
        "    # ======== Create Model ==========\n",
        "    model, opt = get_model()\n",
        "\n",
        "    epochs_progress_bar = tqdm(range(epochs), desc=f\"Epoch: (1/{epochs})\", position=1)\n",
        "    val_loss_global = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch: {epoch+1}\")\n",
        "\n",
        "        # ======================== Training ===============================\n",
        "        train_step_progress_bar = tqdm(total=num_fold_examples, desc=\"Training\", position=2, leave=True)\n",
        "        for step, (images, labels) in enumerate(train_fold):\n",
        "            images = train_pipeline(images)\n",
        "            loss_value = train_step(images, labels)\n",
        "            desc = (\n",
        "                f\"Training loss at step {step}: {round(float(loss_value), 4)}\\n\"\n",
        "            )\n",
        "            gc.collect()\n",
        "            train_step_progress_bar.desc = desc\n",
        "            train_step_progress_bar.update(1)  \n",
        "\n",
        "        train_accuracy = train_acc.result()\n",
        "        print(f\"Training acc over epoch: {round(float(train_accuracy), 4)}\")\n",
        "        train_acc.reset_states()\n",
        "\n",
        "        # ======================== Evaluation ==============================\n",
        "        zo_val_loss, val_loss = [], []\n",
        "        for images, labels in test_fold:\n",
        "            images = test_pipeline(images)\n",
        "            zo_val_loss_step, val_loss_step = test_step(images, labels)\n",
        "            zo_val_loss.append(zo_val_loss_step)\n",
        "            val_loss.append(val_loss_step)\n",
        "        zo_val_loss, val_loss = mean(zo_val_loss), mean(val_loss)\n",
        "\n",
        "        print(\"Zero-One: \", zo_val_loss)\n",
        "        print(\"Val loss: \", val_loss)\n",
        "\n",
        "        val_accuracy = test_acc.result()\n",
        "        test_acc.reset_states()\n",
        "\n",
        "        if val_loss < val_loss_global:\n",
        "            print(f\"Validation loss decreased: {round(val_loss, 4)}\")\n",
        "            val_loss_global = val_loss\n",
        "            model.save(\n",
        "                f\"{ln_args.model_name_or_path}/lenet_epoch_{epoch}_fold_{fold}\"\n",
        "            )\n",
        "\n",
        "        desc = (\n",
        "            f\"Validation acc: {round(float(val_accuracy), 4)} | \"\n",
        "            f\"Time taken: {round(time.time() - start_time, 4)}\"\n",
        "        )\n",
        "\n",
        "        # ======================== Housekeeping ============================\n",
        "        gc.collect()\n",
        "        k.backend.clear_session()\n",
        "\n",
        "        epochs_progress_bar.write(desc)\n",
        "        epochs_progress_bar.desc = desc\n",
        "        epochs_progress_bar.update(1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe2g1NirZNXH"
      },
      "source": [
        "# 📱 Keras **ViT** fine-tuning using HuggingFace\n",
        "\n",
        "Easy implementation with Keras, training it with built-in model.fit\n",
        "\n",
        "Here we don't perform Cross-Validation, it would take too much time to perform several epochs so many times.\n",
        "\n",
        "Paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "Abstract: _**While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited.** In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. **We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train**._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS2JxuG-Z_4l"
      },
      "outputs": [],
      "source": [
        "!pip install transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YVaxpI9ntMK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard as TensorboardCallback, EarlyStopping\n",
        "from transformers import TFViTModel\n",
        "from tensorflow_addons.optimizers import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad3A0f8AZwkE"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ViTrainingArgs:\n",
        "    model_name_or_path: str = \"google/vit-base-patch16-224-in21k\"\n",
        "    monitor_best: str = \"val_loss\"\n",
        "    epochs: int = 5\n",
        "    log_dir: str = \"vit/logdir/\"\n",
        "    num_labels: int = 2\n",
        "\n",
        "vit_args = ViTrainingArgs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVFYdx29kF8V"
      },
      "source": [
        "We could define our network as previously done, inheriting from Keras Model. Keras allow us to define our NN with a more compact form (litterally the same thing in Flax - which is JAX based, the framework in which ViTs were originally trained - is called [@nn.compact](). We can go through model layers and pass our data through them. It will automatically create the computation graph. Since it was trained with images in a channel_first flavour, we need to permute our channel_last images before feeding the network.\n",
        "\n",
        "```python\n",
        "class ViTImageClassifier(Model):\n",
        "    def __init__(\n",
        "        self, \n",
        "        name: str\n",
        "    ):\n",
        "        self.permute = tf.keras.layers.Permute((3, 1, 2))\n",
        "        self.pretrained = TFViTModel.from_pretrained(\n",
        "            name,\n",
        "            add_pooling_layer=False\n",
        "        )\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            units=vit_args.num_labels,\n",
        "            kernel_initializer=\"he_uniform\",\n",
        "            activation='softmax',\n",
        "            name=\"classifier\"\n",
        "        )\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.permute(x)\n",
        "        x = self.pretrained.vit(x)[0]\n",
        "        x = self.classifier(x[:, 0, :])\n",
        "        return x\n",
        "```\n",
        "\n",
        "This can be written as:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzRPFuq6ZQmP"
      },
      "outputs": [],
      "source": [
        "base_model = TFViTModel.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    add_pooling_layer=False\n",
        ")\n",
        "\n",
        "input = tf.keras.layers.Input(shape=(224, 224, 3), name='pixel_values', dtype='float32')\n",
        "channel_first = tf.keras.layers.Permute((3, 1, 2))(input)\n",
        "vit = base_model.vit(channel_first)[0]\n",
        "classifier = tf.keras.layers.Dense(\n",
        "    units=vit_args.num_labels,\n",
        "    kernel_initializer=\"he_uniform\",\n",
        "    activation='softmax',\n",
        "    name=\"classifier\"\n",
        ")(vit[:, 0, :])\n",
        "\n",
        "model = tf.keras.Model(inputs=input, outputs=classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLpFz0SktisO"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    TensorboardCallback(log_dir=vit_args.log_dir)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcY3gcSLvydh"
      },
      "outputs": [],
      "source": [
        "num_train_steps = train_ds.cardinality().numpy() * vit_args.epochs\n",
        "\n",
        "lr_schedule = tf.optimizers.schedules.ExponentialDecay(1e-4, 100, 0.9)\n",
        "wd_schedule = tf.optimizers.schedules.ExponentialDecay(5e-5, 100, 0.9)\n",
        "\n",
        "optimizer = AdamW(learning_rate=lr_schedule, weight_decay=lambda: None)\n",
        "optimizer.weight_decay = lambda: wd_schedule(optimizer.iterations)\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "model.compile(\n",
        "    optimizer = optimizer,\n",
        "    loss = loss,\n",
        "    metrics = accuracy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqRu4Q753zqm"
      },
      "source": [
        "Images values must be in the range _[-1, 1]_ for ViT. So the Rescaling layer is different here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVi5__Bw28hl"
      },
      "outputs": [],
      "source": [
        "train_pipeline = tf.keras.Sequential(\n",
        "    [   \n",
        "        tf.keras.layers.RandomZoom(\n",
        "            height_factor=0.4, width_factor=0.4\n",
        "        ),\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(0.04), \n",
        "        tf.keras.layers.Normalization(mean=0., variance=1.),\n",
        "        tf.keras.layers.Rescaling(scale=1./127.5, offset=-1)  \n",
        "    ]\n",
        ")\n",
        "\n",
        "test_pipeline = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.Normalization(mean=0., variance=1.),\n",
        "        tf.keras.layers.Rescaling(scale=1./127.5, offset=-1)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeQz_Zsi2zkc"
      },
      "outputs": [],
      "source": [
        "vit_train_ds = train_ds.map(\n",
        "    lambda x, y: (\n",
        "        train_pipeline(x, training=True), y), \n",
        "    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "vit_test_ds = test_ds.map(\n",
        "    lambda x, y: (\n",
        "        test_pipeline(x, training=False), y), \n",
        "    num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-ixvfFepnCV0"
      },
      "outputs": [],
      "source": [
        "#@title Remove ViT Folder and models\n",
        "#@markdown If you need to clean up output folder.\n",
        "!rm -rf vit/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QFDGefnPvcvq"
      },
      "outputs": [],
      "source": [
        "#@title ViT Tensorboard \n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir vit/logdir/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdDYJ696rKkD"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    vit_train_ds,\n",
        "    validation_data=vit_test_ds,\n",
        "    callbacks=callbacks,\n",
        "    epochs=vit_args.epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlZjrhoafY_D"
      },
      "source": [
        "# 🎺 **ConvNeXt** \n",
        "\n",
        "We're gonna implement **step-by-step** a ConvNeXt model and train it on _Cats_vs_Dogs_, using pre-trained weights.\n",
        "\n",
        "Paper: https://arxiv.org/abs/2201.03545\n",
        "\n",
        "Abstract: _**The “Roaring 20s” of visual recognition began with the\n",
        "introduction of Vision Transformers (ViTs), which quickly\n",
        "superseded ConvNets as the state-of-the-art image classification model**. A vanilla ViT, on the other hand, faces difficulties\n",
        "when applied to general computer vision tasks such as object\n",
        "detection and semantic segmentation. **It is the hierarchical\n",
        "Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable\n",
        "as a generic vision backbone and demonstrating remarkable\n",
        "performance on a wide variety of vision tasks**. However,\n",
        "the effectiveness of such hybrid approaches is still largely\n",
        "credited to the intrinsic superiority of Transformers, rather\n",
        "than the inherent inductive biases of convolutions. In this\n",
        "work, we reexamine the design spaces and test the limits of\n",
        "what a pure ConvNet can achieve. **We gradually “modernize”\n",
        "a standard ResNet toward the design of a vision Transformer,\n",
        "and discover several key components that contribute to the\n",
        "performance difference along the way. The outcome of this\n",
        "exploration is a family of pure ConvNet models dubbed ConvNeXt**. Constructed entirely from standard ConvNet modules,\n",
        "ConvNeXts compete favorably with Transformers in terms of\n",
        "accuracy and scalability, **achieving 87.8% ImageNet top-1\n",
        "accuracy and outperforming Swin Transformers on COCO\n",
        "detection and ADE20K segmentation, while maintaining the\n",
        "simplicity and efficiency of standard ConvNets**._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKSQvAgH85sO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install split-folders\n",
        "!pip install torch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCb8u9twfbjj"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import splitfolders\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Optional, Tuple\n",
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHafJjF5fN87"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ConvNeXtDataArguments:\n",
        "    input_folder: str = 'dataset/Cats_vs_Dogs'\n",
        "    dataset_folder: str = 'dataset/Cat_vs_Dogs_split'\n",
        "    split_ratio: Tuple[float] = (0.9, 0.1)\n",
        "    seed: int = 1337\n",
        "    num_labels: int = 2\n",
        "\n",
        "cn_args = ConvNeXtDataArguments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWKAVUmN8bo5"
      },
      "outputs": [],
      "source": [
        "cats_vs_dogs(data_args, get_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNEkk0jIfFhi"
      },
      "outputs": [],
      "source": [
        "splitfolders.ratio(\n",
        "    cn_args.input_folder, \n",
        "    output=cn_args.dataset_folder, \n",
        "    seed=cn_args.seed, \n",
        "    ratio=cn_args.split_ratio\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check\n",
        "train_cats = next(os.walk(cn_args.dataset_folder + \"/train/Cat\"))[2]\n",
        "train_dogs = next(os.walk(cn_args.dataset_folder + \"/train/Dog\"))[2]\n",
        "val_cats = next(os.walk(cn_args.dataset_folder + \"/val/Cat\"))[2]\n",
        "val_dogs = next(os.walk(cn_args.dataset_folder + \"/val/Dog\"))[2]\n",
        "print(\"Train Cats: \", len(train_cats))\n",
        "print(\"Train Dogs: \", len(train_dogs))\n",
        "print(\"Val Cats: \", len(val_cats))\n",
        "print(\"Val Dogs: \", len(val_dogs))"
      ],
      "metadata": {
        "id": "fbqC7Zy2eMvm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XipLoNTQgI9c"
      },
      "source": [
        "## ⚛ Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMrUe-wG7g_L"
      },
      "source": [
        "We can find model implementations in HuggingFace [Transformers](https://github.com/huggingface/transformers) or in [Timm](https://github.com/rwightman/pytorch-image-models) (Pytorch Image Models) from Ross Wightman. This implementation is heavily inspired from those, but we clean the code reducing possible clutter. We'll go step by step explaining all the sections. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_path = \"./dataset/Cats_vs_Dogs/\"\n",
        "\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "image_data = torchvision.datasets.ImageFolder(\n",
        "  root=data_path, transform=transform_img\n",
        ")\n",
        "\n",
        "image_data_loader = DataLoader(\n",
        "  image_data, \n",
        "  batch_size=32, \n",
        "  shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "ilPfdTkRhHAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = []\n",
        "std = []\n",
        "\n",
        "for inputs, labels in tqdm(image_data_loader):\n",
        "    mean.append(inputs.mean([0,2,3]).numpy())\n",
        "    std.append(inputs.std([0,2,3]).numpy())\n",
        "    \n",
        "print(\"Mean: \", np.mean(mean, axis=0))\n",
        "print(\"Std: \", np.mean(std, axis=0))"
      ],
      "metadata": {
        "id": "qfb6tWDgiox8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If training from scratch, use cats_vs_dogs, if fine-tuning use imagenet**"
      ],
      "metadata": {
        "id": "f3VeKVb1vuUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHOKmEW2mrYs"
      },
      "outputs": [],
      "source": [
        "# catsvsdogs\n",
        "preprocessor_config = {\n",
        "    \"crop_pct\": 0.875,\n",
        "    \"do_normalize\": True,\n",
        "    \"do_resize\": True,\n",
        "    \"image_mean\": [0.49034384, 0.45506233, 0.4163233],\n",
        "    \"image_std\": [0.2585075, 0.2513996, 0.25332677],\n",
        "    \"resample\": 3,\n",
        "    \"size\": 224\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imagenet1k\n",
        "preprocessor_config = {\n",
        "    \"crop_pct\": 0.875,\n",
        "    \"do_normalize\": True,\n",
        "    \"do_resize\": True,\n",
        "    \"image_mean\": [0.485, 0.456, 0.406],\n",
        "    \"image_std\": [\n",
        "        0.229,\n",
        "        0.224,\n",
        "        0.225\n",
        "    ],\n",
        "    \"resample\": 3,\n",
        "    \"size\": 224\n",
        "}"
      ],
      "metadata": {
        "id": "7jBc_b9eYUKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnbCQHpbRhGT"
      },
      "source": [
        "Pre-trained model is hosted on Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvtObyEApWQr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Choose pre-trained\n",
        "model_name = \"convnext-base-224-22k\" #@param [\"convnext-base-224-22k-1k\", \"convnext-base-224-22k\", \"convnext-tiny-224\"]\n",
        "url = f\"https://huggingface.co/facebook/{model_name}\"\n",
        "\n",
        "os.system(\"git lfs install\")\n",
        "print(\"Downloading the model...\")\n",
        "_ = os.system(f\"git clone {url}\")\n",
        "print(\"Model downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfQjrOcbQBZr"
      },
      "source": [
        ">**Stochastic Depth**\n",
        "\n",
        "**DropPath** (Stochastic Depth) regularization layers: https://arxiv.org/abs/1603.09382. It is adapted from the _timm_ repository.\n",
        "\n",
        "Regularization often yields better generalization properties by avoiding overfitting. \n",
        "\n",
        "From the paper:\n",
        "_**Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks**. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. **The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time**. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. **It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation**. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91% on CIFAR-10)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sb1A457gKQB"
      },
      "outputs": [],
      "source": [
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Stochastic depth\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.p = drop_prob\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training or self.p == 0.0:\n",
        "            return x\n",
        "\n",
        "        keep_prob = 1. - self.p\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(*shape, dtype=x.dtype).to(x.device)\n",
        "        random_tensor.floor_()  # binarize\n",
        "        x = x.div(keep_prob) * random_tensor\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ik36QNSfJa"
      },
      "source": [
        ">**Layer Normalization** \n",
        "\n",
        "From the paper we read that they replaced every Batch Normalization layer with Layer Normalization. \n",
        "\n",
        "From the paper:\n",
        "_BatchNorm is an essential component in ConvNets as it improves the convergence\n",
        "and reduces overfitting. However, BN also has many intricacies that can have a detrimental effect on the model’s\n",
        "performance. There have been numerous attempts at\n",
        "developing alternative normalization techniques,\n",
        "but BN has remained the preferred option in most vision\n",
        "tasks. On the other hand, the simpler Layer Normalization (LN) has been used in Transformers, resulting in\n",
        "good performance across different application scenarios.\n",
        "**Directly substituting LN for BN in the original ResNet\n",
        "will result in suboptimal performance**. With all the modifications in network architecture and training techniques,\n",
        "here we revisit the impact of using LN in place of BN. **We\n",
        "observe that our ConvNet model does not have any difficulties training with LN; in fact, the performance is slightly\n",
        "better, obtaining an accuracy of 81.5%**.\n",
        "From now on, we will use one LayerNorm as our choice\n",
        "of normalization in each residual block._\n",
        "\n",
        "Layer Normalization: https://arxiv.org/abs/1607.06450\n",
        "\n",
        "How it works?\n",
        "\n",
        "$$y_i=\\lambda(\\frac{x_i-\\mu}{\\sqrt{\\sigma^2+\\epsilon}})+\\beta$$\n",
        "\n",
        "Gamma and Beta are two parameters, Epsilon is a constant. Specifically, given a sample of shape [N, C, H, W] LayerNorm calculates a mean and variance of all the elements of shape [C, H, W] in each batch. This method not only solves both problems mentioned above, but also removes the requirement for storing mean and variances for inference (something which Batch Normalization layers needs to do during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJ7QC-tJRPcQ"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    LayerNorm is a normalization layer:\n",
        "    it normalizes the activations of the previous layer for each example.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "        self.features = (features,)\n",
        "        self.data_format = data_format\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.data_format == \"channels_last\":\n",
        "            x = F.layer_norm(x, self.features, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLNy39BWctWs"
      },
      "source": [
        "From Swim Transformer. Images are splitted in patches to feed the network.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aZCDjO-88iV"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    This class is comparable to (and inspired by) the SwinEmbeddings class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embeddings = nn.Conv2d(\n",
        "            config.num_channels,\n",
        "            config.hidden_sizes[0],\n",
        "            kernel_size=config.patch_size,\n",
        "            stride=config.patch_size,\n",
        "        )\n",
        "        self.layer_norm = LayerNorm(config.hidden_sizes[0], eps=1e-6, data_format=\"channels_first\")\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param x: pixel_values (batch_size, height, width, num_channels)\n",
        "        :return: embeddings (batch_size, height, width, hidden_size)\n",
        "        \"\"\"\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHK25e56fU8I"
      },
      "source": [
        "They used larger kernel sizes _(7x7)_.\n",
        "\n",
        "From the paper:\n",
        "_We experimented with several kernel sizes, including 3, 5, 7, 9, and 11. The network’s performance increases\n",
        "from 79.9% (3×3) to 80.6% (7×7), while the network’s\n",
        "FLOPs stay roughly the same._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p-t_IFS9AsC"
      },
      "outputs": [],
      "source": [
        "class ConvNeXtLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a Block of the ConvNeXt model.\n",
        "\n",
        "    - [DwConv, Permute to (N, H, W, C), LayerNorm (channels_last), Linear, GELU, Linear]; Permute back\n",
        "\n",
        "    This implementation according to HuggingFace's engineers is faster.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, dim, drop_path=0):\n",
        "        super().__init__()\n",
        "        self.dw_conv = nn.Conv2d(\n",
        "            dim,\n",
        "            dim,\n",
        "            kernel_size=7,\n",
        "            padding=3,\n",
        "            groups=dim,\n",
        "        )\n",
        "        self.layer_norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pw_conv1 = nn.Linear(dim, dim * 4)\n",
        "        self.act = nn.GELU()\n",
        "        self.pw_conv2 = nn.Linear(dim * 4, dim)\n",
        "        self.layer_scale = nn.Parameter(\n",
        "            config.layer_scale_init_value * torch.ones((dim)), requires_grad=True\n",
        "        )\n",
        "\n",
        "        self.drop_path = DropPath(drop_prob=drop_path) if drop_path else nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param x: (N, C, H, W), we convert it in (N, H, W, C)\n",
        "        :return: (N, C, H, W)\n",
        "        \"\"\"\n",
        "        input = x\n",
        "        x = self.dw_conv(x)\n",
        "        # (N, H, W, C)\n",
        "        x = x.permute(0, 2, 3, 1)  \n",
        "        x = self.layer_norm(x)\n",
        "        x = self.pw_conv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pw_conv2(x)\n",
        "        if self.layer_scale is not None:\n",
        "            x = x * self.layer_scale\n",
        "\n",
        "        # repermute to (N, C, H, W)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "\n",
        "        x = input + self.drop_path(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcFljC7B9Dcu"
      },
      "outputs": [],
      "source": [
        "class ConvNeXtStage(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a Stage of the ConvNeXt model.\n",
        "    We add an optional downsampling step and multiple residual blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=2,\n",
        "        stride=2,\n",
        "        depth=2,\n",
        "        drop_path_rates: List[float] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels or stride > 1:\n",
        "            self.downsample = nn.Sequential(\n",
        "                LayerNorm(in_channels, eps=1e-6, data_format=\"channels_first\"),\n",
        "                nn.Conv2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=stride,\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                ConvNeXtLayer(\n",
        "                    config, out_channels, drop_path=drop_path_rates[i]\n",
        "                ) for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n",
        "        x = self.downsample(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykhm910g9Mss"
      },
      "outputs": [],
      "source": [
        "class ConvNeXtEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Hi I'm the ConvNeXt Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        drop_path_rates = [\n",
        "            x.tolist()\n",
        "            for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(\n",
        "                config.depths\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "        previous_channels = config.hidden_sizes[0]\n",
        "\n",
        "        for i in range(config.num_stages):\n",
        "            out_channels = config.hidden_sizes[i]\n",
        "            stage = ConvNeXtStage(\n",
        "                config,\n",
        "                in_channels=previous_channels,\n",
        "                out_channels=out_channels,\n",
        "                drop_path_rates=drop_path_rates[i],\n",
        "                stride=2 if i > 0 else 1,\n",
        "                depth=config.depths[i]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            previous_channels = out_channels\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        for layer in self.stages:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07cLIDoWhFRa"
      },
      "outputs": [],
      "source": [
        "class ConvNeXtModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = Embeddings(config)\n",
        "        self.encoder = ConvNeXtEncoder(config)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_sizes[-1], eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.FloatTensor = None\n",
        "    ):\n",
        "        if x is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        embedding_output = self.embeddings(x)\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output\n",
        "        )\n",
        "\n",
        "        o_pooled = self.layernorm(encoder_outputs.mean([-2, -1]))\n",
        "        return o_pooled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nvwpfokkyJG"
      },
      "source": [
        "Let's define our Classifier. Add a Linear Layer on top to perform classification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TwdyIaJ9VVE"
      },
      "outputs": [],
      "source": [
        "class ImageClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the ConvNext model for image classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, num_labels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.convnext = ConvNeXtModel(config)\n",
        "        self.classifier = nn.Linear(config.hidden_sizes[-1], self.num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        labels: Optional[torch.LongTensor] = None\n",
        "    ) -> torch.FloatTensor:\n",
        "\n",
        "        x = self.convnext(pixel_values)\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ConvNeXtTrainingArgs:\n",
        "    epochs = 50\n",
        "    learning_rate = 1e-3\n",
        "    warmup_steps = 0\n",
        "    weight_decay = 1e-8\n",
        "\n",
        "cn_train = ConvNeXtTrainingArgs()"
      ],
      "metadata": {
        "id": "5_nZfHzIaJwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4dUx82Ya6Ja"
      },
      "source": [
        "## Pre-trained weights 🙀🔥\n",
        "Here we load pre-trained weights adapting them for our model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWKTzsKYRu7V"
      },
      "outputs": [],
      "source": [
        "class clsprops:\n",
        "\tdef __init__(self, data):\n",
        "\t\tself.__dict__ = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QF_5XF-jktT"
      },
      "outputs": [],
      "source": [
        "class ConvNeXtClassifier:\n",
        "    \"\"\"\n",
        "    A class to load pretrained checkpoints into our model\n",
        "    \"\"\"\n",
        "    @classmethod\n",
        "    def clean_state_dict(cls, model, state_dict):\n",
        "        model_fields = set(k for k in model.state_dict().keys())\n",
        "        model_state_dict = model.state_dict()\n",
        "        hf_field_names = state_dict.keys()\n",
        "        unused_fields = set(hf_field_names)\n",
        "        remap = {}\n",
        "\n",
        "        mismatched_fields = []\n",
        "        for field_name in hf_field_names:\n",
        "            new_name = field_name.replace(\"pwconv\", \"pw_conv\")\n",
        "            new_name = new_name.replace(\"dwconv\", \"dw_conv\")\n",
        "            new_name = new_name.replace(\"layernorm\", \"layer_norm\")\n",
        "            new_name = new_name.replace(\"downsampling_layer\", \"downsample\")\n",
        "            new_name = new_name.replace(\"layer_scale_parameter\", \"layer_scale\")\n",
        "            new_name = new_name.replace(\"layernorm\", \"layer_norm\")\n",
        "\n",
        "            # since it is from nn it is called layernorm \n",
        "            new_name = new_name.replace(\"convnext.layer_norm.bias\", \"convnext.layernorm.bias\")\n",
        "            new_name = new_name.replace(\"convnext.layer_norm.weight\", \"convnext.layernorm.weight\")\n",
        "\n",
        "            if new_name in model_fields:\n",
        "                # cleans last linear layer, if there's a mismatch, skip\n",
        "                # we don't use the ImageNet pretrained head\n",
        "                if state_dict[field_name].shape != model_state_dict[new_name].shape:\n",
        "                    mismatched_fields.append(new_name)\n",
        "                else:\n",
        "                    model_fields.remove(new_name)\n",
        "                    unused_fields.remove(field_name)\n",
        "                    remap[new_name] = state_dict[field_name]\n",
        "            \n",
        "        model.load_state_dict(remap, strict=False)\n",
        "        return model_fields, unused_fields, mismatched_fields\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, checkpoint_dir: str, map_location=None, num_labels=2, **kwargs):\n",
        "        if os.path.isdir(checkpoint_dir):\n",
        "            checkpoint = os.path.join(checkpoint_dir, 'pytorch_model.bin')\n",
        "            config = json.load(open(os.path.join(checkpoint_dir, 'config.json')))\n",
        "            config = clsprops(config)\n",
        "            \n",
        "        else:\n",
        "            checkpoint = checkpoint_dir\n",
        "        hf_dict = torch.load(checkpoint, map_location=map_location)\n",
        "\n",
        "        model = ImageClassifier(config, num_labels=num_labels)\n",
        "        mfn, ucf, msf = ConvNeXtClassifier.clean_state_dict(model, hf_dict)\n",
        "\n",
        "        print(\"Pre-trained ConvNeXt loaded! 🔥\")\n",
        "        print(\"Unused pre-trained fields: \", len(ucf))\n",
        "        print(\"Pretrained Head weights not loaded: \", msf)\n",
        "        return model\n",
        "    \n",
        "    @classmethod\n",
        "    def from_scratch(cls, checkpoint_dir: str, map_location=None, num_labels=2, **kwargs):\n",
        "        if os.path.isdir(checkpoint_dir):\n",
        "            config = json.load(open(os.path.join(checkpoint_dir, 'config.json')))\n",
        "            config = clsprops(config)\n",
        "            config.drop_path_rate = 0.3\n",
        "\n",
        "        model = ImageClassifier(config, num_labels=num_labels)\n",
        "        \n",
        "        def _init_weights(module):\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "            elif isinstance(module, nn.LayerNorm):\n",
        "                module.bias.data.zero_()\n",
        "                module.weight.data.fill_(1.0)\n",
        "\n",
        "        model.apply(_init_weights)\n",
        "        print(\"From-scrach ConvNeXt! 🔥\")\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**choose your model:** \n",
        "\n",
        "if you want to fine-tune choose from_pretrained, else from_scratch"
      ],
      "metadata": {
        "id": "aliou0niwE8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCETRLSVy7iO"
      },
      "outputs": [],
      "source": [
        "model = ConvNeXtClassifier.from_pretrained(f\"./{model_name}\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNeXtClassifier.from_scratch(f\"./{model_name}\", num_labels=2)"
      ],
      "metadata": {
        "id": "AoXZ5jRafTS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyD4HtwbnU03"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjEriMs_gsDQ"
      },
      "source": [
        "## : Dataset \n",
        "\n",
        "We can, from Tf data, convert images to numpy, convert them into torch Tensors, but it sounds crazy and it's more practical to use torch functions, that make our life a little easier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJngBdWv_YNR"
      },
      "outputs": [],
      "source": [
        "train_dir = cn_args.dataset_folder + \"/train\"\n",
        "val_dir = cn_args.dataset_folder + \"/val\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBdz6N3m634Q"
      },
      "outputs": [],
      "source": [
        "data_files = {}\n",
        "data_files[\"train\"] = os.path.join(train_dir, \"**\")\n",
        "data_files[\"validation\"] = os.path.join(val_dir, \"**\")\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"imagefolder\",\n",
        "    data_files=data_files,\n",
        "    task=\"image-classification\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In5jFdPNcBtr"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fagh4_N_6u__"
      },
      "outputs": [],
      "source": [
        "labels = dataset[\"train\"].features[\"labels\"].names\n",
        "\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0xqUKW4U8Dc"
      },
      "outputs": [],
      "source": [
        "print(f\"Labels: {labels}\")\n",
        "print(f\"Id of Cat: {label2id['Cat']}\")\n",
        "print(f\"Id of Dog: {label2id['Dog']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEhpPJxJ89_6"
      },
      "source": [
        "**Image Augmentation and Normalization**\n",
        "\n",
        "As previously done, we apply image augmentation and normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taaTPbf9A7kH"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZfi6uFdAz3X"
      },
      "outputs": [],
      "source": [
        "normalize = Normalize(mean=preprocessor_config[\"image_mean\"], std=preprocessor_config[\"image_std\"])\n",
        "_train_transforms = Compose(\n",
        "    [\n",
        "        RandomResizedCrop(preprocessor_config[\"size\"]),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "_val_transforms = Compose(\n",
        "    [\n",
        "        Resize(preprocessor_config[\"size\"]),\n",
        "        CenterCrop(preprocessor_config[\"size\"]),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "def train_transforms(example_batch):\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        _train_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "def val_transforms(example_batch):\n",
        "    example_batch[\"pixel_values\"] = [_val_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[\"image\"]]\n",
        "    return example_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmKx9NuhBhOD"
      },
      "outputs": [],
      "source": [
        "# Dataset.set_transform() allows you to apply a custom formatting transform on-the-fly.\n",
        "# https://huggingface.co/docs/datasets/process\n",
        "# Transforms are applied only when the images are accessed.\n",
        "\n",
        "dataset[\"train\"].set_transform(train_transforms)\n",
        "dataset[\"validation\"].set_transform(train_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HMI8mctdCrA"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=32\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    dataset[\"validation\"], collate_fn=collate_fn, batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q31L-oaG9L1O"
      },
      "source": [
        "## 🗻 **Optimizer**\n",
        "\n",
        "As optimizer, we use AdamW or Adam or ..., making sure that biases and layer normalization are not considered during optimization if we fine-tune. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md1kr03edfGV"
      },
      "outputs": [],
      "source": [
        "def get_parameter_names(model, forbidden_layer_types):\n",
        "    result = []\n",
        "    for name, child in model.named_children():\n",
        "        result += [\n",
        "            f\"{name}.{n}\"\n",
        "            for n in get_parameter_names(child, forbidden_layer_types)\n",
        "            if not isinstance(child, tuple(forbidden_layer_types))\n",
        "        ]\n",
        "    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n",
        "    result += list(model._parameters.keys())\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inipWJPoNt1V"
      },
      "outputs": [],
      "source": [
        "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
        "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
        "        \"weight_decay\": cn_train.weight_decay,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
        "        \"weight_decay\": 0.,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**choose optimizer:**"
      ],
      "metadata": {
        "id": "sdW49rHtvW5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two implementations of AdamW, AdamP, Adam and Shampoo. In the experiments Adam is used. For fine-tuning model.parameters() must be replaced with optimizer_grouped_parameters, since we don't want to train layernorm etc. If training from scratch, leave it."
      ],
      "metadata": {
        "id": "oG6kM_Drvbh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4sPCrSESco3"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), \n",
        "    lr = 4e-5,\n",
        "    betas = (0.9, 0.999),\n",
        "    eps = 1e-08\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=4e-5, weight_decay = 0.001)"
      ],
      "metadata": {
        "id": "le660Gv-njuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obQtqfI5NUaf"
      },
      "outputs": [],
      "source": [
        "import torch_optimizer\n",
        "\n",
        "optimizer = torch_optimizer.AdamP(\n",
        "    model.parameters(), \n",
        "    lr=4e-5, \n",
        "    delta=0.1,\n",
        "    wd_ratio=0.1,\n",
        "    nesterov=False,\n",
        "    weight_decay=5e-3, \n",
        "    eps=1e-08\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_optimizer\n",
        "\n",
        "optimizer = torch_optimizer.Shampoo(\n",
        "    model.parameters(), \n",
        "    lr=cn_train.learning_rate\n",
        ")"
      ],
      "metadata": {
        "id": "wWgc7-xh1Alc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd8XG09Cjpbx"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), \n",
        "    lr = cn_train.learning_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear scheduler, can be bypassed**"
      ],
      "metadata": {
        "id": "lmvpOH_uxJ8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYWGjA3UQOmz"
      },
      "outputs": [],
      "source": [
        "# adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py \n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(step: int):\n",
        "        if step < num_warmup_steps:\n",
        "            return float(step) / float(max(1, num_warmup_steps))\n",
        "        return max(\n",
        "            0.0, float(num_training_steps - step) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        )\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-1GDWMYIexL"
      },
      "outputs": [],
      "source": [
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    cn_train.warmup_steps,\n",
        "    int(len(train_dataloader) * cn_train.epochs)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpCOZWav5dH1"
      },
      "source": [
        "## 🚋 **Train** and **Evaluate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8FD1Wg68WJU"
      },
      "source": [
        "**Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS_JphzzKViT"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "loss_fct = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c6cxtnwNhqU"
      },
      "source": [
        "**Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ6GjxKhNg8T"
      },
      "outputs": [],
      "source": [
        "train_metric = load_metric(\"accuracy\")\n",
        "eval_metric = load_metric(\"accuracy\")\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpDm3LBn8ZXh"
      },
      "source": [
        "**Steps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2l4oUIOxkjz"
      },
      "outputs": [],
      "source": [
        "def train_step(batch):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    \n",
        "\n",
        "    # pass the batch to the model \n",
        "    outputs = model(**batch)\n",
        "\n",
        "    # compute the loss function \n",
        "    loss = loss_fct(outputs.view(-1, cn_args.num_labels), batch[\"labels\"].view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    # lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # update the metric\n",
        "    predictions = outputs.argmax(dim=-1)\n",
        "    train_metric.add_batch(\n",
        "        predictions = predictions,\n",
        "        references = batch[\"labels\"],\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "def test_step(batch):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "        loss = loss_fct(outputs.view(-1, cn_args.num_labels), batch[\"labels\"].view(-1))\n",
        "    \n",
        "    # get the scores for the batch \n",
        "    # tensor {0, 1} * batch_size\n",
        "    predictions = outputs.argmax(dim=-1)\n",
        "\n",
        "    eval_metric.add_batch(\n",
        "        predictions = predictions, \n",
        "        references = batch[\"labels\"]\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZF3EP9O8b9O"
      },
      "source": [
        "**Training**: \n",
        "\n",
        "Adapting here our Keras training script to PyTorch, it's quite the same. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl_UoxXXJE57"
      },
      "outputs": [],
      "source": [
        "#@title ConvNeXt Tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiQEILzMnDQd"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(epochs, train_dataloader, eval_dataloader):\n",
        "    num_train_examples = len(train_dataloader)\n",
        "    num_eval_examples = len(eval_dataloader)\n",
        "\n",
        "    global_eval_accuracy = 0.\n",
        "    epochs_progress_bar = tqdm(range(epochs), desc=f\"Epoch: (1/{epochs})\", position=0)\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch: {epoch+1}\")\n",
        "\n",
        "        # ======================== Training ===============================\n",
        "        model.train()\n",
        "        train_step_progress_bar = tqdm(total=num_train_examples, desc=\"Training\", position=1, leave=True)\n",
        "\n",
        "        for _, batch in enumerate(train_dataloader):\n",
        "            loss = train_step(batch)\n",
        "            writer.add_scalar(\"Train Loss\", float(loss), step)\n",
        "            step += 1\n",
        "\n",
        "            desc = (\n",
        "                f\"Training loss at step {step}: {round(float(loss), 4)}\\n\"\n",
        "            )\n",
        "            train_step_progress_bar.desc = desc\n",
        "            train_step_progress_bar.update(1)  \n",
        "\n",
        "        train_acc = train_metric.compute()\n",
        "        train_acc = train_acc[\"accuracy\"]\n",
        "        print(f\"Train accuracy: {round(train_acc, 4)}\")\n",
        "        writer.add_scalar(\"Train Accuracy\", train_acc, epoch)\n",
        "\n",
        "        # ======================== Evaluation ==============================\n",
        "        model.eval()\n",
        "        eval_step_progress_bar = tqdm(total=num_eval_examples, desc=\"Eval\", position=2, leave=True)\n",
        "        eval_losses = []\n",
        "        for _, batch in enumerate(eval_dataloader):\n",
        "            eval_loss = test_step(batch)\n",
        "            eval_losses.append(eval_loss.cpu().numpy())\n",
        "\n",
        "            eval_step_progress_bar.desc = desc\n",
        "            eval_step_progress_bar.update(1)\n",
        "        \n",
        "        eval_loss = np.mean(eval_losses)\n",
        "        writer.add_scalar(\"Eval Loss\", eval_loss, epoch)\n",
        "        print(\"Eval loss: \", round(float(eval_loss), 4))\n",
        "\n",
        "        eval_acc = eval_metric.compute()\n",
        "        eval_acc = eval_acc[\"accuracy\"]\n",
        "        print(f\"Eval accuracy: {round(eval_acc, 4)}\")\n",
        "        writer.add_scalar(\"Eval Accuracy\", eval_acc, epoch)\n",
        "\n",
        "        if eval_acc > global_eval_accuracy:\n",
        "            global_eval_accuracy = eval_acc\n",
        "            torch.save(model, \"convnext_pytorch.bin\")\n",
        "\n",
        "        epochs_progress_bar.update(1) \n",
        "    writer.flush()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(\n",
        "    cn_train.epochs,\n",
        "    train_dataloader, \n",
        "    eval_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "3dLquPQnfNUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glgNCq5DR6v5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title delete **model** - clean **cache**\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5_58K8dOMSn"
      },
      "outputs": [],
      "source": [
        "#@title Train with **Huggingface** Trainer\n",
        "\n",
        "!python run_image_classification.py \\\n",
        "    --model_name_or_path ./convnext-base-224-22k-1k \\\n",
        "    --train_dir ./dataset/Cat_vs_Dogs_split/train \\\n",
        "    --validation_dir ./dataset/Cat_vs_Dogs_split/val \\\n",
        "    --output_dir ./trainer_convnext/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --ignore_mismatched_sizes True \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 8 \\\n",
        "    --per_device_train_batch_size 32 \\\n",
        "    --per_device_eval_batch_size 32 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 50 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --seed 1337"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-91F7QuaPB2i",
        "xQlSk1nPfTeG",
        "xe2g1NirZNXH"
      ],
      "name": "CDImageClassification.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}