# Can new architectures reach public SOTA?

- Statistical Methods for Machine Learning - UniMi 2022
<p align="center">
  <img src="https://user-images.githubusercontent.com/49756048/179430266-ff45e619-b880-4ef1-b855-32970af00c34.png" alt="drawing" width="500"/>
</p>

_Since 2017, Transformers have revolutionized everything Deep Learning related. First applied to text (e.g., BERT-like, T5-like, and GPT-like architectures), then successfully to images (e.g., ViT, Swin Transformer, DeIT etc.), and even on graphs (e.g., Graph Transformer), CNNs have been left behind in terms of performance. With ConvNeXt, EfficientNet(V2) and other new architectures, the research community has tried to apply to ResNets some of the pieces that characterize Transformers, reaching state-of-the-art on ImageNet22k with all-CNN-based NNs. Can these new architectures help us reach SOTA on the Cats vs. Dogs Image Classification dataset?_
